{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de400ab",
   "metadata": {},
   "source": [
    "# LLM Evaluation with Arize AI, OpenAI, and Phoenix\n",
    "\n",
    "This notebook runs a batch of prompts through OpenAI's GPT-3.5 model, evaluates them using GPT-4 via Phoenix's `QAEvaluator`, and traces activity with Arize AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4f5ef",
   "metadata": {},
   "source": [
    "üì¶ Installation Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai arize openinference-instrumentation-openai arize-phoenix-evals pandas arize-otel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba885837",
   "metadata": {},
   "source": [
    "üß© Imports and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad941870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arize.otel import register\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from config import openai_api_key, arizeai_api_key, arizeai_proj_name, arizeai_spid\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "from phoenix.evals import QAEvaluator, OpenAIModel, run_evals\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112221ca",
   "metadata": {},
   "source": [
    "üîß Logging and OpenAI Setup (Code Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4580da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set OpenAI API Key\n",
    "openai.api_key = openai_api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d3677",
   "metadata": {},
   "source": [
    "üîÅ Arize Tracer Registration (Code Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c25a9282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Arize Project: My LLM Project\n",
      "|  Span Processor: BatchSpanProcessor\n",
      "|  Collector Endpoint: otlp.arize.com\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'authorization': '****', 'api_key': '****', 'arize-space-id': '****', 'space_id': '****', 'arize-interface': '****', 'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the tracer provider with Arize\n",
    "tracer_provider = register(\n",
    "    space_id=arizeai_spid,\n",
    "    api_key=arizeai_api_key,\n",
    "    project_name=arizeai_proj_name\n",
    ")\n",
    "\n",
    "# Instrument the OpenAI client\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "client = openai.OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04998f6",
   "metadata": {},
   "source": [
    "üìù Prompt Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77bc9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = [\n",
    "    {\"input\": \"What is the capital of France?\", \"reference\": \"Paris\"},\n",
    "    {\"input\": \"Who wrote the novel '1984'?\", \"reference\": \"George Orwell\"},\n",
    "    {\"input\": \"What is NYC's most famous landmark?\", \"reference\": \"Statue of Liberty\"},\n",
    "    {\"input\": \"What language is primarily spoken in Brazil?\", \"reference\": \"Portuguese\"},\n",
    "    {\"input\": \"What is the tallest mountain in the world?\", \"reference\": \"Mount Everest\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02bba7",
   "metadata": {},
   "source": [
    "ü§ñ Batch Prompt Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec26d2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Sending prompt: What is the capital of France?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Response: The capital of France is Paris.\n",
      "INFO:root:Sending prompt: Who wrote the novel '1984'?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Response: George Orwell wrote the novel '1984'.\n",
      "INFO:root:Sending prompt: What is NYC's most famous landmark?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Response: The most famous landmark in New York City is the Statue of Liberty.\n",
      "INFO:root:Sending prompt: What language is primarily spoken in Brazil?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Response: The primary language spoken in Brazil is Portuguese.\n",
      "INFO:root:Sending prompt: What is the tallest mountain in the world?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Response: Mount Everest, standing at 29,032 feet (8,848 meters) above sea level, is the tallest mountain in the world.\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "# Run all prompts and collect responses\n",
    "for item in prompt_data:\n",
    "    prompt = item[\"input\"]\n",
    "    reference = item[\"reference\"]\n",
    "    try:\n",
    "        logging.info(f\"Sending prompt: {prompt}\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=50\n",
    "        )\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        logging.info(f\"Response: {response_text}\")\n",
    "        results.append({\n",
    "            \"input\": prompt,\n",
    "            \"output\": response_text,\n",
    "            \"reference\": reference,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during OpenAI request for prompt '{prompt}': {e}\")\n",
    "        results.append({\n",
    "            \"input\": prompt,\n",
    "            \"output\": \"Error\",\n",
    "            \"reference\": reference,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404e823",
   "metadata": {},
   "source": [
    "üìä Create DataFrame (Code Cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f71fc249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>reference</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>The capital of France is Paris.</td>\n",
       "      <td>Paris</td>\n",
       "      <td>1.748353e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who wrote the novel '1984'?</td>\n",
       "      <td>George Orwell wrote the novel '1984'.</td>\n",
       "      <td>George Orwell</td>\n",
       "      <td>1.748353e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is NYC's most famous landmark?</td>\n",
       "      <td>The most famous landmark in New York City is t...</td>\n",
       "      <td>Statue of Liberty</td>\n",
       "      <td>1.748353e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What language is primarily spoken in Brazil?</td>\n",
       "      <td>The primary language spoken in Brazil is Portu...</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>1.748353e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the tallest mountain in the world?</td>\n",
       "      <td>Mount Everest, standing at 29,032 feet (8,848 ...</td>\n",
       "      <td>Mount Everest</td>\n",
       "      <td>1.748353e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input  \\\n",
       "0                What is the capital of France?   \n",
       "1                   Who wrote the novel '1984'?   \n",
       "2           What is NYC's most famous landmark?   \n",
       "3  What language is primarily spoken in Brazil?   \n",
       "4    What is the tallest mountain in the world?   \n",
       "\n",
       "                                              output          reference  \\\n",
       "0                    The capital of France is Paris.              Paris   \n",
       "1              George Orwell wrote the novel '1984'.      George Orwell   \n",
       "2  The most famous landmark in New York City is t...  Statue of Liberty   \n",
       "3  The primary language spoken in Brazil is Portu...         Portuguese   \n",
       "4  Mount Everest, standing at 29,032 feet (8,848 ...      Mount Everest   \n",
       "\n",
       "      timestamp  \n",
       "0  1.748353e+09  \n",
       "1  1.748353e+09  \n",
       "2  1.748353e+09  \n",
       "3  1.748353e+09  \n",
       "4  1.748353e+09  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c978d5ef",
   "metadata": {},
   "source": [
    "üìà Run QA Evaluation with GPT-4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d84ffd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phoenix.evals.executors:üêå!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e46e3818d04391849a64ae987a91a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "run_evals |          | 0/5 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'score', 'explanation'], dtype='object')\n",
      "     label  score                                        explanation\n",
      "0  correct      1  The question asks for the capital of France. T...\n",
      "1  correct      1  The question asks who wrote the novel '1984'. ...\n",
      "2  correct      1  The answer correctly identifies the Statue of ...\n",
      "3  correct      1  The question asks about the primary language s...\n",
      "4  correct      1  The question asks for the tallest mountain in ...\n"
     ]
    }
   ],
   "source": [
    "# Run QA Evaluation with GPT-4\n",
    "eval_model = OpenAIModel(model=\"gpt-4\", api_key=openai_api_key)\n",
    "qa_evaluator = QAEvaluator(eval_model)\n",
    "\n",
    "qa_eval_dfs = run_evals(\n",
    "    dataframe=df,\n",
    "    evaluators=[qa_evaluator],\n",
    "    provide_explanation=True\n",
    ")\n",
    "\n",
    "# Extract and show results\n",
    "qa_eval_df = qa_eval_dfs[0]\n",
    "print(qa_eval_df.columns)\n",
    "print(qa_eval_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7346661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prompt 1 ---\n",
      "Input      : What is the capital of France?\n",
      "Output     : The capital of France is Paris.\n",
      "Reference  : Paris\n",
      "Label      : correct\n",
      "Score      : 1\n",
      "Explanation: The question asks for the capital of France. The reference text provides the answer as 'Paris'. The given answer states 'The capital of France is Paris.' which matches the information provided in the reference text. Therefore, the answer is correct.\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Input      : Who wrote the novel '1984'?\n",
      "Output     : George Orwell wrote the novel '1984'.\n",
      "Reference  : George Orwell\n",
      "Label      : correct\n",
      "Score      : 1\n",
      "Explanation: The question asks who wrote the novel '1984'. The reference text provides the name 'George Orwell'. The answer states that 'George Orwell wrote the novel '1984', which is in line with the information provided in the reference text. Therefore, the answer is correct.\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Input      : What is NYC's most famous landmark?\n",
      "Output     : The most famous landmark in New York City is the Statue of Liberty.\n",
      "Reference  : Statue of Liberty\n",
      "Label      : correct\n",
      "Score      : 1\n",
      "Explanation: The answer correctly identifies the Statue of Liberty as the most famous landmark in New York City, which matches the reference text. Therefore, the answer is correct.\n",
      "\n",
      "--- Prompt 4 ---\n",
      "Input      : What language is primarily spoken in Brazil?\n",
      "Output     : The primary language spoken in Brazil is Portuguese.\n",
      "Reference  : Portuguese\n",
      "Label      : correct\n",
      "Score      : 1\n",
      "Explanation: The question asks about the primary language spoken in Brazil. The reference text provides the answer as 'Portuguese'. The given answer states that 'The primary language spoken in Brazil is Portuguese.' which is in line with the reference text. Therefore, the given answer is correct.\n",
      "\n",
      "--- Prompt 5 ---\n",
      "Input      : What is the tallest mountain in the world?\n",
      "Output     : Mount Everest, standing at 29,032 feet (8,848 meters) above sea level, is the tallest mountain in the world.\n",
      "Reference  : Mount Everest\n",
      "Label      : correct\n",
      "Score      : 1\n",
      "Explanation: The question asks for the tallest mountain in the world. The reference text provides the answer as Mount Everest. The given answer also states that Mount Everest is the tallest mountain in the world and provides additional information about its height. Therefore, the given answer correctly answers the question based on the reference text.\n"
     ]
    }
   ],
   "source": [
    "# Combine original columns with evaluation results\n",
    "combined_df = pd.concat(\n",
    "    [df[[\"input\", \"output\", \"reference\"]].reset_index(drop=True), qa_eval_df.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "for index, row in combined_df.iterrows():\n",
    "    print(f\"\\n--- Prompt {index + 1} ---\")\n",
    "    print(f\"Input      : {row['input']}\")\n",
    "    print(f\"Output     : {row['output']}\")\n",
    "    print(f\"Reference  : {row['reference']}\")\n",
    "    print(f\"Label      : {row['label']}\")\n",
    "    print(f\"Score      : {row['score']}\")\n",
    "    print(f\"Explanation: {row['explanation']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
